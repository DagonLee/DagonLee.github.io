# RNN의 소개

---

## 시퀀스 데이터

- 순서가 있는 데이터
- ex) 단어, 소리, 영상, 주가 등

### 시퀀스 데이터 다루기

- 아래와 같이 조건부확률을 이용하여 시퀀스 데이터를 이용할 수 있다.

    ![RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.34.10.png](RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.34.10.png)

    Xt-1 ~ X1 까지 발생했을 때 Xt가 발생할 확률

- 조건부에 들어가는 데이터의 길이가 가변적이기 때문에 이를 고려해야한다.

    ![RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.41.16.png](RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.41.16.png)

- 과거의 모든 데이터를 그대로 사용한다면 부담이 되기 때문에 고정된 길이만큼의 데이터를 다루는 모델도 있다(Autoregressive Model, 자기회귀모델)

    ![RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.42.17.png](RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.42.17.png)

- 다른 방법으로는 과거의 데이터들을 잠재벡터로 연산하여 이용하는 방법이 있다 ⇒ RNN

    ![RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.49.59.png](RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.49.59.png)

- RNN의 기본적인 모형을 보면 아래와 같다.

    ![RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.02.37.png](RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.02.37.png)

    $H_{t}$는 $X_{t + 1}$와 함께 $H_{t+1}$를 연산하는 입력으로 들어간다. 과거의 데이터가 현재에 영향을 주는 모델이다. 

- 수식으로 나타내면 다음과 같다.

    ![RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.12.43.png](RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.12.43.png)

    과거의 연산결과인 $H_{t-1}$, $X_t$ 가 각각의 가중치와 연산되어 $H_t$를 만드는 요소가 되고 연산된

    $H_t$는 또 다른 가중치와 연산되어 ouput인 $O_t$를 반환한다. 3개의 가중치를 가지고 있다.

- BPTT(Back Propagation Through Time)

    RNN의 Back Propagation은 시간의 흐름 대로  RNN에 적용되는 BPTT방법을 이용하여 Back Propagation을 진행한다.

    ![RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_6.41.41.png](RNN%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20a84c86efca394799983a617f9bc13d2c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_6.41.41.png)

    시퀀스의 길이가 길어질수록 gradient가 증강 되거나 소실 될 수 있다.

    ⇒ truncated bptt(길이를 제한하여 bptt 진행)

    ⇒ LSTM, GRU라는 대안도 존재